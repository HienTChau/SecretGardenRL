{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4TqiyiP031x2hBIXcjcTW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HienTChau/SecretGardenRL/blob/main/SecretGarden.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Secret Garden\n",
        "Solving a Cookie Run Kingdom minigame with Reinforcement Learning"
      ],
      "metadata": {
        "id": "9XOJTP0OFtfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries"
      ],
      "metadata": {
        "id": "atVSHEUbkP2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Software Installation"
      ],
      "metadata": {
        "id": "bAcmnKVPG0Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# remove legacy gym (installed in Colab by default)\n",
        "!pip uninstall -y gym\n",
        "\n",
        "# environments\n",
        "! pip install \"gymnasium[box2d, atari, mujoco]\"\n",
        "\n",
        "! sudo apt install swig # needed for box2d\n",
        "\n",
        "# RL algorithms\n",
        "! pip install \"stable-baselines3[extra]\"\n",
        "\n",
        "# visualization\n",
        "! apt-get update && apt-get install ffmpeg freeglut3-dev xvfb"
      ],
      "metadata": {
        "id": "jWItUW15E46t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "1q-yakdjG232"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stable_baselines3\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "\n",
        "import os\n",
        "import re\n",
        "import base64\n",
        "\n",
        "from time import sleep\n",
        "from pathlib import Path\n",
        "\n",
        "# for pretty display in notebook\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
        "\n",
        "# suppresses deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# display library versions\n",
        "print(f'stable_baselines3 version: {stable_baselines3.__version__}')\n",
        "print(f'gym version: {gym.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoaH2vKAE8Mx",
        "outputId": "7203a7e5-65f6-4ee0-e383-c8d0c0d8b032"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stable_baselines3 version: 2.7.0\n",
            "gym version: 1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Code"
      ],
      "metadata": {
        "id": "nFavTUBUG8xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dirs(dirs):\n",
        "  for dir in dirs:\n",
        "    if not os.path.exists(dir):\n",
        "      os.makedirs(dir)\n",
        "      print(f\"Created directory: {dir}\")\n",
        "\n",
        "def create_env_instance(env_name, log_dir=None, wrap=True, **env_kwargs):\n",
        "  env = gym.make(env_name, **env_kwargs)\n",
        "\n",
        "  # wraps the environment to provide additional functionality\n",
        "  if wrap:\n",
        "    env = Monitor(env, filename=log_dir)\n",
        "\n",
        "  return env\n",
        "\n",
        "def show_env_info(env):\n",
        "  spec = gym.spec(env.unwrapped.spec.id)\n",
        "\n",
        "  print(f'Environment Name: {spec.id}')\n",
        "  print(f'Action Space: {env.action_space}')\n",
        "  print(f'Observation Space: {env.observation_space}')\n",
        "  print(f'Max Episode Steps: {spec.max_episode_steps}')\n",
        "  print(f'Nondeterministic: {spec.nondeterministic}')\n",
        "\n",
        "def get_session_name(env, algorithm):\n",
        "    \"\"\"\n",
        "    Build a filesystem-friendly session name from an environment and an\n",
        "    SB3 algorithm instance.\n",
        "\n",
        "    Example output: \"CartPole-v1_PPO\"\n",
        "    \"\"\"\n",
        "    if isinstance(env, str):\n",
        "        env_name = env\n",
        "    else:\n",
        "        # handle vectorized envs (DummyVecEnv, SubprocVecEnv, etc.)\n",
        "        base_env = env\n",
        "        if hasattr(env, \"envs\") and len(env.envs) > 0:\n",
        "            base_env = env.envs[0]\n",
        "\n",
        "        # unwrap if needed (Monitor, TimeLimit, etc.)\n",
        "        base_env = getattr(base_env, \"unwrapped\", base_env)\n",
        "\n",
        "        # try spec.id, fall back to class name\n",
        "        spec = getattr(base_env, \"spec\", None)\n",
        "        env_name = getattr(spec, \"id\", None) or type(base_env).__name__\n",
        "\n",
        "    # get algorithm name from instance\n",
        "    alg_name = type(algorithm).__name__   # e.g., \"PPO\", \"TD3\", \"SAC\"\n",
        "\n",
        "    # combine and make filesystem-safe\n",
        "    raw_name = f\"{env_name}_{alg_name}\"\n",
        "    safe_name = re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", raw_name)\n",
        "\n",
        "    return safe_name\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    def __init__(self, check_freq: int, log_dir: str, save_dir: str, save_filename: str, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_dir = save_dir\n",
        "        self.save_path = os.path.join(save_dir, save_filename)\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "            # retrieve training reward\n",
        "            x, y = results_plotter.ts2xy(\n",
        "                results_plotter.load_results(self.log_dir), \"timesteps\")\n",
        "            if len(x) > 0:\n",
        "                # mean training reward over the last 100 episodes\n",
        "                mean_reward = np.mean(y[-100:])\n",
        "                if self.verbose > 0:\n",
        "                    clear_output(wait=True)\n",
        "\n",
        "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                    print(\n",
        "                        f\"Previous best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
        "                    )\n",
        "\n",
        "                # new best model, you could save the agent here\n",
        "                if mean_reward > self.best_mean_reward:\n",
        "                    self.best_mean_reward = mean_reward\n",
        "                    # example for saving best model\n",
        "                    if self.verbose > 0:\n",
        "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                    self.model.save(self.save_path)\n",
        "\n",
        "        return True\n",
        "\n",
        "def record_video(env_id, model, video_path, prefix=\"rl-video\", video_length=500):\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_path,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    eval_env.close()\n",
        "\n",
        "def show_videos(video_path, prefix=\"rl-video\"):\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(f\"{prefix}*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "metadata": {
        "id": "UkN_9Y3DHAb7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive Configuration"
      ],
      "metadata": {
        "id": "T_R1Ow7XHiSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mounts your Google Drive as a directory in your Colab virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# creates the top-level directory where all Google Drive content will be saved\n",
        "drive_basedir='/content/drive/MyDrive/comp377/colab'\n",
        "os.makedirs(drive_basedir, exist_ok=True)\n",
        "! ls -R $drive_basedir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FFCkwlmFQ5m",
        "outputId": "3197ad95-d237-426e-bf69-ae24ee03c8be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/comp377/colab:\n",
            "figures  models  videos\n",
            "\n",
            "/content/drive/MyDrive/comp377/colab/figures:\n",
            "\n",
            "/content/drive/MyDrive/comp377/colab/models:\n",
            "CartPole-v1_PPO.zip\n",
            "\n",
            "/content/drive/MyDrive/comp377/colab/videos:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Configuration"
      ],
      "metadata": {
        "id": "_gcTtRK1HoBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contains temporary files created during training\n",
        "train_dir = '/tmp/gym/model/train/'\n",
        "tensorboard_dir = '/tmp/gym/tensorboard'\n",
        "\n",
        "# directories that will contain content generated during training\n",
        "model_dir  = os.path.join(drive_basedir, 'models')\n",
        "video_dir  = os.path.join(drive_basedir, 'videos')\n",
        "figure_dir = os.path.join(drive_basedir, 'figures')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "os.makedirs(figure_dir, exist_ok=True)\n",
        "\n",
        "# set up fake display; otherwise video rendering will fail\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "metadata": {
        "id": "IiROnW24FYwm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "QTjmr-c7HvQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project requires a customized environment in Gymnasium.\n",
        "\n",
        "Each game grants the board a 4x4 tile board. Players can place a building/block on a random empty space on the board. Blocks Lv. 1, 2, and 3 each have a chance to be placed on the board, 60%, 25%, and 15% resprectively, with the next block to be placed being visible.\n",
        "\n",
        "Blocks can be merged with an adjacent block of the same level to create a higher-level block at the new location. Each new or merged block grants points equal to **2 to the power of the block's level - 1**. Blocks can be leveled up to Lv. 12 maximum.\n",
        "\n",
        "Creating any Lv. block for the first time in the game unlocks a reward for Lv.4 blocks and above\n",
        "Items can be used at any time as long as the game hasn't ended.\n",
        "\n",
        "- Rewind: Undoes the last merge, or removes the last placed building. Can only be used 3 times between actions.\n",
        "- Hammer: Destroys and frees up a block.\n",
        "- Blueprint: Places the next building on a chosen tile."
      ],
      "metadata": {
        "id": "9_yt08w6JEe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Level | Rewards (modified) |\n",
        "|-------|---------|\n",
        "| 1 | ‚Äî |\n",
        "| 2 | ‚Äî |\n",
        "| 3 | ‚Äî |\n",
        "| 4 | Hammer x5 |\n",
        "| 5 | Rewind x1 |\n",
        "| 6 | Blueprint x1 |\n",
        "| 7 | - |\n",
        "| 8 | Hammer x5 |\n",
        "| 9 | Rewind x1 |\n",
        "| 10 | Blueprint x1 |\n",
        "| 11 | - |\n",
        "| 12 | - |"
      ],
      "metadata": {
        "id": "qSfaqj8cJeHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Design Questions**\n",
        "\n",
        "<ul>\n",
        "üéØ Skill: Select the optimal action\n",
        "\n",
        "üëÄ Information: The current state of the board, next tile and available items\n",
        "\n",
        "üéÆ Actions: Skip, Merge (Hammer, Rewind, Blueprint will be added later)\n",
        "\n",
        "üèÜ Success: Achieve maximum score or create the highest level block in minimum steps\n",
        "\n",
        "‚è∞ End: When there is no available move (or optional time limit)\n",
        "<ul>\n"
      ],
      "metadata": {
        "id": "3FUK8sDfKKyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SecretGardenEnv(gym.Env):\n",
        "  def __init__(self, size: int = 4):\n",
        "    # The size of the square grid\n",
        "    self.size = size\n",
        "    self.n_cells = size * size\n",
        "\n",
        "    self._board = None\n",
        "    self._next_block = None\n",
        "    self._score = 0\n",
        "\n",
        "    # Observation space\n",
        "    self.observation_space = gym.spaces.Dict({\n",
        "      'board': gym.spaces.Box(low=0, high=12, shape=(size, size), dtype=np.uint8),\n",
        "      \"next_block\": gym.spaces.Discrete(4) # will use 1, 2, 3\n",
        "      # 'items': gym.spaces.Box(low=0, high=1, shape=(3,), dtype=np.uint8) #this will be added later\n",
        "    })\n",
        "\n",
        "    # Action space\n",
        "    # 0 = PLACE\n",
        "    # 1‚Äì256 = MERGE(i, j)\n",
        "    self.action_space = gym.spaces.Discrete(1 + self.n_cells * self.n_cells)\n",
        "\n",
        "    self._action_map = {}\n",
        "    for i in range(self.n_cells):\n",
        "      for j in range(self.n_cells):\n",
        "        self._action_map[1 + i * self.n_cells + j] = (i, j)\n",
        "\n",
        "  def generate_next_block(self):\n",
        "    return np.random.choice(\n",
        "        [1,2,3],\n",
        "        p=[0.6, 0.25, 0.15]\n",
        "    )\n",
        "\n",
        "  def _get_obs(self):\n",
        "    \"\"\"Convert internal state to observation format.\n",
        "\n",
        "    Returns:\n",
        "        dict: Observation with current board and next block\n",
        "    \"\"\"\n",
        "    return {\"board\": self._board, \"next_block\": self._next_block}\n",
        "\n",
        "  def _get_info(self):\n",
        "    \"\"\"Compute auxiliary information for debugging.\n",
        "\n",
        "    Returns:\n",
        "        dict: Info with the current score\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"score\": self._score,\n",
        "    }\n",
        "\n",
        "  def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "    \"\"\"Start a new episode.\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed for reproducible episodes\n",
        "        options: Additional configuration (unused in this example)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (observation, info) for the initial state\n",
        "    \"\"\"\n",
        "    # Seed the random number generator\n",
        "    super().reset(seed=seed)\n",
        "\n",
        "    # Initialize the board\n",
        "    self._board = np.zeros((self.size, self.size), dtype=np.uint8)\n",
        "\n",
        "    # Generate the next block\n",
        "    self._next_block = self.generate_next_block()\n",
        "\n",
        "    # Set score\n",
        "    self._score = 0\n",
        "\n",
        "    # Return the initial observation and info\n",
        "    observation = self._get_obs()\n",
        "    info = self._get_info()\n",
        "\n",
        "    return observation, info\n",
        "\n",
        "  def _get_empty_cells(self):\n",
        "    \"\"\"Returns a list of empty cells on the board.\n",
        "\n",
        "    Returns:\n",
        "        list: List of empty cells\n",
        "    \"\"\"\n",
        "    empty_cells = []\n",
        "    for i in range(self.size):\n",
        "      for j in range(self.size):\n",
        "        if self._board[i, j] == 0:\n",
        "          empty_cells.append((i, j))\n",
        "    return empty_cells\n",
        "\n",
        "  def _has_valid_merge(self):\n",
        "    \"\"\"Checks if there is a valid merge on the board.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if there is a valid merge, False otherwise\n",
        "    \"\"\"\n",
        "    for i in range(self.size):\n",
        "      for j in range(self.size):\n",
        "        cur = self._board[i, j]\n",
        "        if cur == 0:\n",
        "          continue\n",
        "        # Check right\n",
        "        if j + 1 < self.size and self._board[i, j + 1] == cur:\n",
        "            return True\n",
        "        # Check down\n",
        "        if i + 1 < self.size and self._board[i + 1, j] == cur:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "  def _is_terminal(self):\n",
        "    \"\"\"Checks if the game is over.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the game is over, False otherwise\n",
        "    \"\"\"\n",
        "    return not self._has_valid_merge() and not self._get_empty_cells()\n",
        "\n",
        "  def _is_valid_merge(self, ax, ay, bx, by):\n",
        "    \"\"\"Checks if a merge is valid.\"\"\"\n",
        "\n",
        "    # A and B must be adjacent\n",
        "    if abs(ax - bx) + abs(ay - by) != 1:\n",
        "        return False\n",
        "\n",
        "    lvlA = self._board[ax, ay]\n",
        "    lvlB = self._board[bx, by]\n",
        "\n",
        "    # Both must be nonzero and same level\n",
        "    if lvlA == 0 or lvlB == 0:\n",
        "        return False\n",
        "    if lvlA != lvlB:\n",
        "        return False\n",
        "\n",
        "    # Cannot merge at max level\n",
        "    if lvlA >= 12:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Execute one time step within the environment.\n",
        "\n",
        "    Args:\n",
        "        action: Action to be executed\n",
        "\n",
        "    Returns:\n",
        "        tuple: (observation, reward, terminated, truncated, info)\n",
        "    \"\"\"\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    reward = 0\n",
        "\n",
        "    # 1. PLACE ACTION\n",
        "\n",
        "    if action == 0:\n",
        "      # Get empty cells\n",
        "      empty_cells = self._get_empty_cells()\n",
        "\n",
        "      if not empty_cells:\n",
        "        return self._get_obs(), reward, terminated, truncated, self._get_info()\n",
        "\n",
        "      # Place the block in a random empty cell\n",
        "      i, j = empty_cells[np.random.randint(len(empty_cells))]\n",
        "      self._board[i, j] = self._next_block\n",
        "\n",
        "      # Reward for placing\n",
        "      reward += 1\n",
        "\n",
        "      self._score += reward\n",
        "      self._next_block = self.generate_next_block()\n",
        "\n",
        "    # 2. MERGE ACTION\n",
        "    elif action > 0:\n",
        "      # Tile A is the source, tile B is the destination\n",
        "      A, B = self._action_map[action]\n",
        "\n",
        "      # Decode tiles into 2D pos\n",
        "      ax, ay = divmod(A, self.size)\n",
        "      bx, by = divmod(B, self.size)\n",
        "\n",
        "      if self._is_valid_merge(ax, ay, bx, by):\n",
        "        # Merge the blocks\n",
        "        self._board[bx, by] += 1\n",
        "        self._board[ax, ay] = 0\n",
        "\n",
        "        # Reward for merging\n",
        "        reward += 2 ** (self._board[bx, by] - 1)\n",
        "\n",
        "        self._score += reward\n",
        "\n",
        "    if self._is_terminal():\n",
        "      terminated = True\n",
        "\n",
        "    return self._get_obs(), reward, terminated, truncated, self._get_info()\n",
        "\n",
        "  def render(self, mode=\"human\"):\n",
        "    print(\"Score:\", self._score)\n",
        "    print(\"Next block:\", self._next_block)\n",
        "    print(self._board)\n",
        "    print(\"-\" * 20)\n",
        ""
      ],
      "metadata": {
        "id": "vin5AyLeLvWH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SecretGardenEnv()\n",
        "obs, info = env.reset()\n",
        "env.render()\n",
        "\n",
        "obs, reward, done, trunc, info = env.step(0)  # PLACE\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfdfFJlSXCbr",
        "outputId": "d79c9f28-8569-4d80-9320-31b3eb127406"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0\n",
            "Next block: 1\n",
            "[[0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]]\n",
            "--------------------\n",
            "Score: 1\n",
            "Next block: 2\n",
            "[[0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 0 0]]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQNRL4ckZX8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}